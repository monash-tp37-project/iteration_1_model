{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install all packages needed\n",
    "\n",
    "#!pip3 install sklearn --upgrade\n",
    "#!pip3 install pickle --update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/arminberger/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/arminberger/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import all libraries needed\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import os\n",
    "import spacy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import langid\n",
    "\n",
    "# nltk used for parsing and cleaning text\n",
    "import nltk\n",
    "import unicodedata\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from difflib import SequenceMatcher\n",
    "from scipy import spatial\n",
    "from itertools import combinations\n",
    "\n",
    "# used to acccess the sql database\n",
    "import pymysql\n",
    "# library that helps turn dataframes into sql tables\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "'''# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import Phrases\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models'''\n",
    "\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import pickle\n",
    "\n",
    "## for deep learning\n",
    "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/arminberger/Documents/GitHub/iteration_1_model'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Read in Data from MySQl database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create connection\n",
    "connection = pymysql.connect(host='news-data-rdb.cqsnaejqwcpu.ap-southeast-2.rds.amazonaws.com',\n",
    "                             user='admin',\n",
    "                             password='badpassword1',\n",
    "                             db='NewsData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query to retreive our data for the model\n",
    "SQL_Query = pd.read_sql_query(\n",
    "    '''select\n",
    "        *\n",
    "        from labeled_news_data''', connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data rereived in a query as a pandas dataframe\n",
    "news_sql_df = pd.DataFrame(SQL_Query, columns=['news_id', 'text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>house dem aide didnt even see comeys letter ja...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ever get feeling life circle roundabout rather...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>truth might get fired october tension intellig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>video civilian killed single u airstrike ident...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>print iranian woman sentenced six year prison ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19871</th>\n",
       "      <td>19876</td>\n",
       "      <td>rapper unloaded black celebrity met donald tru...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19872</th>\n",
       "      <td>19877</td>\n",
       "      <td>green bay packer lost washington redskin week ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19873</th>\n",
       "      <td>19878</td>\n",
       "      <td>macys today grew union several great name amer...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19874</th>\n",
       "      <td>19879</td>\n",
       "      <td>nato russia hold parallel exercise balkan 11 0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19875</th>\n",
       "      <td>19880</td>\n",
       "      <td>david swanson author activist journalist radio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19876 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       news_id                                               text  label\n",
       "0            0  house dem aide didnt even see comeys letter ja...      1\n",
       "1            1  ever get feeling life circle roundabout rather...      0\n",
       "2            2  truth might get fired october tension intellig...      1\n",
       "3            3  video civilian killed single u airstrike ident...      1\n",
       "4            4  print iranian woman sentenced six year prison ...      1\n",
       "...        ...                                                ...    ...\n",
       "19871    19876  rapper unloaded black celebrity met donald tru...      0\n",
       "19872    19877  green bay packer lost washington redskin week ...      0\n",
       "19873    19878  macys today grew union several great name amer...      0\n",
       "19874    19879  nato russia hold parallel exercise balkan 11 0...      1\n",
       "19875    19880  david swanson author activist journalist radio...      1\n",
       "\n",
       "[19876 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the result\n",
    "news_sql_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Vectorize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_text = news_sql_df.text.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer = 'word',\n",
    "                             input = 'content',\n",
    "                            lowercase = True,\n",
    "                            token_pattern = '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "                            min_df = 3,\n",
    "                            ngram_range = (1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the training text input \n",
    "x_train = vectorizer.fit_transform(x_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the target data as a list\n",
    "y_train = news_sql_df.label.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'vectorizer.pk'\n",
    "pickle.dump(vectorizer, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical model used in this assignement\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the LogisticRegression using our training data\n",
    "model = model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'basic_news_logistic_regression.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIONAL TEST - Step 4. Predict the news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"user_news_input = input('Paste your text here: ')\\nuser_news_input_processed = futher_process_string(user_news_input)\\nuser_news_input_processed = ' '.join(user_news_input_processed)\\nuser_news_input_vec = vectorizer.transform([user_news_input_processed])\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''user_news_input = input('Paste your text here: ')\n",
    "user_news_input_processed = futher_process_string(user_news_input)\n",
    "user_news_input_processed = ' '.join(user_news_input_processed)\n",
    "user_news_input_vec = vectorizer.transform([user_news_input_processed])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# prediction of our target\\nprediction = model.predict(user_news_input_vec)\\n\\nif prediction[0] == 1:\\n    print('The news is likely to be UNRELIABLE!')\\nelse:\\n    print('The news is likely to be RELIABLE!')\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# prediction of our target\n",
    "prediction = model.predict(user_news_input_vec)\n",
    "\n",
    "if prediction[0] == 1:\n",
    "    print('The news is likely to be UNRELIABLE!')\n",
    "else:\n",
    "    print('The news is likely to be RELIABLE!')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.random.randint(2, size=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
