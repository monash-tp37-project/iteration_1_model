{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install all packages needed\n",
    "\n",
    "#!pip install gensim\n",
    "#!pip install pyLDAvis\n",
    "#!pip3 install openpyxl --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/arminberger/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/arminberger/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import all libraries needed\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import os\n",
    "import openpyxl\n",
    "#import jupyter_resource_usage\n",
    "import re\n",
    "import math\n",
    "import collections\n",
    "import spacy\n",
    "import de_core_news_sm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import langid\n",
    "\n",
    "# nltk used for parsing and cleaning text\n",
    "import nltk\n",
    "import unicodedata\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from difflib import SequenceMatcher\n",
    "from scipy import spatial\n",
    "from itertools import combinations\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import Phrases\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "\n",
    "## for deep learning\n",
    "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/arminberger/Desktop/Code/fit5120_code'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data overview:\n",
    "\n",
    "train.csv: A full training dataset with the following attributes:\n",
    "\n",
    "- id: unique id for a news article\n",
    "- title: the title of a news article\n",
    "- author: author of the news article\n",
    "- text: the text of the article; could be incomplete\n",
    "- label: a label that marks the article as potentially unreliable\n",
    "\n",
    "1: unreliable\n",
    "0: reliable\n",
    "\n",
    "test.csv: A testing training dataset with all the same attributes at train.csv without the label.\n",
    "\n",
    "submit.csv: A sample submission that you can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all three data sets\n",
    "\n",
    "train = pd.read_csv(f'{current_dir }/data_fake_news/kaggle_news_dataset/train.csv')\n",
    "\n",
    "test = pd.read_csv(f'{current_dir }/data_fake_news/kaggle_news_dataset/test.csv')\n",
    "\n",
    "submit = pd.read_csv(f'{current_dir }/data_fake_news/kaggle_news_dataset/submit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20800 entries, 0 to 20799\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      20800 non-null  int64 \n",
      " 1   title   20242 non-null  object\n",
      " 2   author  18843 non-null  object\n",
      " 3   text    20761 non-null  object\n",
      " 4   label   20800 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 812.6+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5200 entries, 0 to 5199\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      5200 non-null   int64 \n",
      " 1   title   5078 non-null   object\n",
      " 2   author  4697 non-null   object\n",
      " 3   text    5193 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 162.6+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20795</th>\n",
       "      <td>Rapper T. I. unloaded on black celebrities who...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20796</th>\n",
       "      <td>When the Green Bay Packers lost to the Washing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20797</th>\n",
       "      <td>The Macy’s of today grew from the union of sev...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20798</th>\n",
       "      <td>NATO, Russia To Hold Parallel Exercises In Bal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20799</th>\n",
       "      <td>David Swanson is an author, activist, journa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      House Dem Aide: We Didn’t Even See Comey’s Let...      1\n",
       "1      Ever get the feeling your life circles the rou...      0\n",
       "2      Why the Truth Might Get You Fired October 29, ...      1\n",
       "3      Videos 15 Civilians Killed In Single US Airstr...      1\n",
       "4      Print \\nAn Iranian woman has been sentenced to...      1\n",
       "...                                                  ...    ...\n",
       "20795  Rapper T. I. unloaded on black celebrities who...      0\n",
       "20796  When the Green Bay Packers lost to the Washing...      0\n",
       "20797  The Macy’s of today grew from the union of sev...      0\n",
       "20798  NATO, Russia To Hold Parallel Exercises In Bal...      1\n",
       "20799    David Swanson is an author, activist, journa...      1\n",
       "\n",
       "[20800 rows x 2 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train[['text','label']]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    10413\n",
       "0    10387\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: unreliable\n",
    "0: reliable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since not all values are unique we will only keep observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20387, 2)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which checks if text is not in english\n",
    "def text_is_english(text):\n",
    "    \n",
    "    if isinstance(text, str):\n",
    "        \n",
    "        if langid.classify(text)[0]!='en':\n",
    " \n",
    "            return True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if tweet is not in english ans sa\n",
    "train['bool_true'] = train.text.apply(lambda x: text_is_english(x))\n",
    "\n",
    "# get a list of all indicese that need to be dropped\n",
    "drop_index = train.index[train.bool_true == True].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the indicese \n",
    "train.drop(drop_index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the last column\n",
    "train.drop(columns = 'bool_true', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19881, 2)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Preprocess the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list of all stopwords\n",
    "english_stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove unwanted characters from as string\n",
    "def remove_char(text):\n",
    "    \n",
    "    # list of unwated charaters \n",
    "    remove_characters = [\"'\",';', ':', '!', \"*\", '/', '\\\\',\"(\", \")\",\",\",\".\",\"-\", \"&\",\"\\n\",'“','@', '–', '\"', '+', '=', '[',']', '?', '”']\n",
    "    \n",
    "    # loop through all unwated characters \n",
    "    for character in remove_characters:\n",
    "                         \n",
    "        text = text.replace(character, \" \")\n",
    "                         \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that replaces accentuated characters by their non-accentuated counterparts\n",
    "# in a string\n",
    "def remove_accents(text):\n",
    "    \n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "    return \"\".join([c for c in text if not unicodedata.combining(c)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean a string and turn it into a uniform format\n",
    "# we can either keep numbers or remove them\n",
    "def clean_string(text):\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    text = text.lower()\n",
    "        \n",
    "    text = text.replace(\"'\",\"\")\n",
    "    \n",
    "    text = remove_char(text)\n",
    "\n",
    "    text = text.strip(' ')\n",
    "    \n",
    "    text = remove_accents(text)\n",
    "\n",
    "    return text\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_list_unprocessed = train.text.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8494176\n"
     ]
    }
   ],
   "source": [
    "## build a vocab for cleaned/standardized keyword token, then count them \n",
    "\n",
    "# set for all tokens \n",
    "news_tokens = []\n",
    "\n",
    "# loop through each keyword name \n",
    "for i in news_list_unprocessed:\n",
    "    \n",
    "    # only edit strings \n",
    "    if type(i) == str:\n",
    "    \n",
    "        # split based on white spaces and create a list of tokens\n",
    "        tokens = i.split(' ')\n",
    "\n",
    "        # loop through all the tokens\n",
    "        for x in tokens:\n",
    "\n",
    "            # clean the string\n",
    "            x = clean_string(x)\n",
    "             \n",
    "            # check if a token is a stopword or non\n",
    "            if x not in english_stop_words and x is not None:\n",
    "                \n",
    "                # check if a token is a number or larger than 1\n",
    "                if x.isnumeric() == False and len(x) > 1:\n",
    "\n",
    "                    # append the cleaned string\n",
    "                    news_tokens.append(x)\n",
    "\n",
    "    \n",
    "# print lenght\n",
    "print(len(news_tokens))\n",
    "\n",
    "# get the count of each token accross all documents\n",
    "token_frequencey = FreqDist(news_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'said': 79595, 'mr': 66177, 'trump': 43854, 'one': 37093, 'would': 36883, 'people': 34853, 'new': 29502, 'like': 25653, 'also': 25175, 'president': 22947, ...})"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_frequencey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which turns an uncleaned sting containing a number of tokens into a list of cleaned tokens \n",
    "def futher_process_string(text):\n",
    "    \n",
    "    # ensure that the text is in string format\n",
    "    text = str(text)\n",
    "    \n",
    "    # don't keep numbers\n",
    "    if text.isnumeric() == False:\n",
    "        \n",
    "        # split the string into individual tokens\n",
    "        text = text.split(' ')\n",
    "        \n",
    "        # save a list of strings\n",
    "        final_string_list = []\n",
    "        \n",
    "        # loop through all tokens\n",
    "        for token in text:\n",
    "            \n",
    "            # clean the string\n",
    "            token = clean_string(token)\n",
    "            \n",
    "            # don't keep numbers\n",
    "            if token.isnumeric() == False and token not in english_stop_words:\n",
    "\n",
    "                # ensure that the token is not None\n",
    "                if  token is not None and token != '':\n",
    "                    \n",
    "                    # lemmatize the token \n",
    "                    token = WordNetLemmatizer().lemmatize(token)\n",
    "\n",
    "                    # append the cleaned and lemmatized token\n",
    "                    final_string_list.append(token)\n",
    "        \n",
    "        # return the the final string list\n",
    "        return final_string_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 9s, sys: 1.01 s, total: 2min 10s\n",
      "Wall time: 2min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# preprocess all news articles\n",
    "news_list_processed = [futher_process_string(x)  for x in news_list_unprocessed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Vectorize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the word2vec model using the gensim library\n",
    "model = gensim.models.Word2Vec(\n",
    "        news_list_processed,  # corpus used for training\n",
    "        window=300,           # size of the embeddig\n",
    "        min_count=2,          # min token occurance\n",
    "        workers=4)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('democratic', 0.5454925298690796),\n",
       " ('schumer', 0.516161322593689),\n",
       " ('chuck', 0.47959956526756287),\n",
       " ('liberal', 0.47745364904403687),\n",
       " ('pelosi', 0.4707755446434021),\n",
       " ('reid', 0.45023971796035767),\n",
       " ('democrats’', 0.4495120048522949),\n",
       " ('dianne', 0.4466678500175476),\n",
       " ('feinstein', 0.4361237585544586),\n",
       " ('progressive', 0.43353983759880066)]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('democrat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
