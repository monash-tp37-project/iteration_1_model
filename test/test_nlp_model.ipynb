{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Test -NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code taken from https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lime\n",
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for data\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "## for processing\n",
    "import re\n",
    "import nltk\n",
    "## for bag-of-words\n",
    "from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing\n",
    "## for explainer\n",
    "from lime import lime_text\n",
    "## for word embedding\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "## for deep learning\n",
    "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "## for bert language model\n",
    "#import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category': 'CRIME',\n",
       " 'headline': 'There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV',\n",
       " 'authors': 'Melissa Jeltsen',\n",
       " 'link': 'https://www.huffingtonpost.com/entry/texas-amanda-painter-mass-shooting_us_5b081ab4e4b0802d69caad89',\n",
       " 'short_description': 'She left her husband. He killed their children. Just another day in America.',\n",
       " 'date': '2018-05-26'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_dics = []\n",
    "with open('data.json', mode='r', errors='ignore') as json_file:\n",
    "    for dic in json_file:\n",
    "        lst_dics.append( json.loads(dic) )\n",
    "## print the first one\n",
    "lst_dics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Ellen Grills Mark Wahlberg About His Shirtless...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189810</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>The Wonderful Wistful World of Wes Anderson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59956</th>\n",
       "      <td>POLITICS</td>\n",
       "      <td>Watch Obama's Remarks From The 2016 White Hous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193090</th>\n",
       "      <td>TECH</td>\n",
       "      <td>Tao Girls: Taobao, Chinese Shopping Site, To O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72488</th>\n",
       "      <td>POLITICS</td>\n",
       "      <td>Cruz, Rubio Lie Low During Trump Uproar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    y                                               text\n",
       "183     ENTERTAINMENT  Ellen Grills Mark Wahlberg About His Shirtless...\n",
       "189810  ENTERTAINMENT        The Wonderful Wistful World of Wes Anderson\n",
       "59956        POLITICS  Watch Obama's Remarks From The 2016 White Hous...\n",
       "193090           TECH  Tao Girls: Taobao, Chinese Shopping Site, To O...\n",
       "72488        POLITICS            Cruz, Rubio Lie Low During Trump Uproar"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create dtf\n",
    "dtf = pd.DataFrame(lst_dics)\n",
    "## filter categories\n",
    "dtf = dtf[ dtf[\"category\"].isin(['ENTERTAINMENT','POLITICS','TECH']) ][[\"category\",\"headline\"]]\n",
    "## rename columns\n",
    "dtf = dtf.rename(columns={\"category\":\"y\", \"headline\":\"text\"})\n",
    "## print 5 random rows\n",
    "dtf.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Preprocess a string.\n",
    ":parameter\n",
    "    :param text: string - name of column containing text\n",
    "    :param lst_stopwords: list - list of stopwords to remove\n",
    "    :param flg_stemm: bool - whether stemming is to be applied\n",
    "    :param flg_lemm: bool - whether lemmitisation is to be applied\n",
    ":return\n",
    "    cleaned text\n",
    "'''\n",
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "            \n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()\n",
    "    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    lst_stopwords]\n",
    "                \n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "                \n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "            \n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>smith join diplo nicky jam 2018 world cup offi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>hugh grant marries first time age 57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>jim carrey blast castrato adam schiff democrat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>julianna margulies us donald trump poop bag pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Morgan Freeman 'Devastated' That Sexual Harass...</td>\n",
       "      <td>morgan freeman devastated sexual harassment cl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               y                                               text  \\\n",
       "1  ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2  ENTERTAINMENT    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "5  ENTERTAINMENT  Morgan Freeman 'Devastated' That Sexual Harass...   \n",
       "\n",
       "                                          text_clean  \n",
       "1  smith join diplo nicky jam 2018 world cup offi...  \n",
       "2               hugh grant marries first time age 57  \n",
       "3  jim carrey blast castrato adam schiff democrat...  \n",
       "4  julianna margulies us donald trump poop bag pi...  \n",
       "5  morgan freeman devastated sexual harassment cl...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtf[\"text_clean\"] = dtf[\"text\"].apply(lambda x: \n",
    "          utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, \n",
    "          lst_stopwords=lst_stopwords))\n",
    "dtf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split dataset\n",
    "dtf_train, dtf_test = model_selection.train_test_split(dtf, test_size=0.3)\n",
    "## get target\n",
    "y_train = dtf_train[\"y\"].values\n",
    "y_test = dtf_test[\"y\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = dtf_train[\"text_clean\"]\n",
    "\n",
    "## create list of lists of unigrams\n",
    "lst_corpus = []\n",
    "for string in corpus:\n",
    "    lst_words = string.split()\n",
    "    lst_grams = [\" \".join(lst_words[i:i+1]) for i in range(0, len(lst_words), 1)]\n",
    "    lst_corpus.append(lst_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['star', 'war', 'monopoly', 'doesnt', 'exclude', 'rey', 'available', 'mass'],\n",
       " ['san', 'franciscos', 'transit', 'agency', 'promise', 'immigration', 'raid']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_corpus[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
